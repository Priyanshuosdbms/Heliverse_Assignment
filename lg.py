import os
from typing import TypedDict, List
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END

# --- 1. CONFIGURATION ---
# Ensure this model is pulled in Ollama (e.g., run: ollama pull llama3.1)
OLLAMA_MODEL = "llama3.1" 

# --- 2. DEFINE GRAPH STATE ---
# This is the data structure passed between nodes in the graph
class AgentState(TypedDict):
    """
    A state for the LangGraph RAG agent.
    - question: The user's original query.
    - documents: List of retrieved text chunks (str).
    - generation: The final answer generated by the LLM (str).
    """
    question: str
    documents: List[str]
    generation: str

# --- 3. INITIALIZE COMPONENTS ---
try:
    # Initialize the local LLM and Embeddings using Ollama
    local_llm = ChatOllama(model=OLLAMA_MODEL, temperature=0.0)
    # local_embeddings = OllamaEmbeddings(model=OLLAMA_MODEL) 
    # NOTE: Embeddings are not strictly needed for this specific example's dummy retriever, 
    # but you would use them for a real Vector Store setup.
    print(f"âœ… Initialized Ollama with model: {OLLAMA_MODEL}")
except Exception as e:
    print(f"âŒ Error initializing Ollama Chat Model: {e}")
    print("Please ensure Ollama is running and the model is pulled.")
    exit()


# --- 4. GRAPH NODES (FUNCTIONS) ---

def retrieve_node(state: AgentState) -> AgentState:
    """
    Node 1: Retrieves documents based on the user's question.
    In a real application, this would query a Vector Store.
    """
    print("\n--- ðŸ”Ž RETRIEVING DOCUMENTS ---")
    question = state["question"]
    
    # --- Dummy Retriever Implementation ---
    # Replace this with your actual vector store logic (e.g., retriever.get_relevant_documents)
    if "ollama" in question.lower():
        documents = [
            "Ollama is a tool for running large language models locally on your machine.", 
            "It simplifies setup and configuration for models like Llama 3 and Mistral.",
            "It runs on Linux, macOS, and Windows (via WSL)."
        ]
    elif "langgraph" in question.lower():
        documents = [
            "LangGraph is a library for building robust, stateful, multi-step LLM applications.",
            "It allows defining workflows as cycles and graphs, similar to state machines."
        ]
    else:
        documents = ["This is a generic document about AI and local development."]

    print(f"Retrieved {len(documents)} document(s).")
    
    # Update the state with the retrieved documents
    return {"question": question, "documents": documents, "generation": ""}

def generate_node(state: AgentState) -> AgentState:
    """
    Node 2: Generates an answer using the retrieved documents and the LLM.
    Uses the LangChain Expression Language (LCEL) for the RAG chain.
    """
    print("\n--- ðŸ§  GENERATING ANSWER ---")
    question = state["question"]
    documents = state["documents"]
    
    # 1. Prompt Template
    prompt = ChatPromptTemplate.from_messages([
        ("system", "You are a helpful RAG assistant. Use the following context to answer the question. If the context is insufficient, state that you cannot find the answer in the provided documents.\n\nCONTEXT:\n{context}"),
        ("human", "{question}"),
    ])

    # 2. RAG Chain using LCEL
    rag_chain = prompt | local_llm | StrOutputParser()

    # 3. Format context and invoke the chain
    context_str = "\n---\n".join(documents)
    
    generation = rag_chain.invoke({"context": context_str, "question": question})
    
    # Update the state with the final generated answer
    return {"question": question, "documents": documents, "generation": generation}


# --- 5. BUILD AND RUN THE GRAPH ---

def run_rag_agent(query: str):
    """Initializes and runs the LangGraph RAG agent."""
    print(f"--- ðŸš€ Agent Starting: {query} ---")
    
    # 1. Initialize the LangGraph
    workflow = StateGraph(AgentState)

    # 2. Add nodes to the graph
    workflow.add_node("retrieve", retrieve_node)
    workflow.add_node("generate", generate_node)

    # 3. Define the entry point (start node)
    workflow.set_entry_point("retrieve")

    # 4. Define the edges (flow of execution)
    # The flow is simple: Retrieve -> Generate -> End
    workflow.add_edge("retrieve", "generate")
    workflow.add_edge("generate", END)

    # 5. Compile the graph
    app = workflow.compile()

    # 6. Run the agent with the user query
    final_state = app.invoke({"question": query, "documents": [], "generation": ""})

    # 7. Print the final result
    print("\n\n--- ðŸŽ‰ FINAL RESPONSE ---")
    print(final_state["generation"])
    print("-------------------------")

if __name__ == "__main__":
    # Example usage:
    user_query = "What is Ollama and what models does it support?"
    run_rag_agent(user_query)
    
    # Another example:
    user_query_2 = "What is LangGraph used for?"
    run_rag_agent(user_query_2)
